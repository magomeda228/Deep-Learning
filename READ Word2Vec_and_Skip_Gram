Вывод по лабораторной работе: Построение модели Skip-Gram для обработки текста
В этой лабораторной работе мы шаг за шагом реализовали архитектуру Skip-Gram для обработки текстовых данных. Рассмотрим ключевые этапы работы:

1. Предобработка данных

Очистка текста: Мы начали с очистки текста с помощью функции preprocess, которая преобразует текст в список слов, удаляя знаки препинания и редкие слова, 
которые встречаются меньше пяти раз. Это необходимо для уменьшения шума в данных.
Маппинг слов в индексы: Создали два словаря для преобразования слов в индексы и обратно. Слова в тексте были преобразованы в числовые индексы в соответствии
с их частотностью. Наиболее частым словам присваивались более низкие индексы.

2. Субдискретизация слов (Subsampling)
С помощью метода субдискретизации мы отбросили часто встречающиеся слова (например, «the», «of», «and»), 
так как они не несут значимой информации о контексте и могут замедлить обучение модели. Слово отбрасывается с вероятностью, пропорциональной его частоте в тексте.

3. Формирование обучающих пар
Для каждой пары слова в тексте (центральное слово и контекст) сформированы обучающие примеры. Мы реализовали функцию get_target, 
которая выбирает случайные слова в окне вокруг целевого слова.
На основе этой функции мы создали батчи данных, используя метод get_batches, который генерирует пары (входное слово — контекст).

4. Обучение модели Skip-Gram
Модель Skip-Gram была построена с использованием нейронной сети, состоящей из слоя эмбеддингов и выходного линейного слоя. 
Задача модели заключается в том, чтобы на основе вектора слова предсказать его контекст (окружающие слова).
Мы обучили модель с использованием функции потерь CrossEntropyLoss, которая подходит для задач классификации. 
Оптимизация происходила с использованием алгоритма Adam.

В процессе обучения мы периодически проверяли качество модели, используя косинусное сходство между векторами эмбеддингов.
Это позволило нам следить за тем, как хорошо модель объединяет слова с похожими значениями.

5. Визуализация результатов
После обучения модели мы использовали метод T-SNE для визуализации многомерных векторов эмбеддингов. 
Это позволяет увидеть, как векторные представления слов группируются в пространстве, и оценить, насколько семантически схожие слова расположены рядом друг с другом.
Визуализация показала, как слова с похожими значениями (например, "cat" и "dog") оказываются рядом, что подтверждает эффективность обучения модели.

Заключение
В ходе лабораторной работы была построена и обучена модель Skip-Gram для генерации векторных представлений слов (эмбеддингов). 
Использование различных техник, таких как предобработка данных, субдискретизация и обучение с помощью алгоритма Skip-Gram, 
позволило создать эффективные эмбеддинги, которые могут быть использованы в дальнейшем для решения других задач NLP, таких как классификация, 
извлечение именованных сущностей и генерация текста.
