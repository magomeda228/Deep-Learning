\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Лабораторная работа: Построение модели Skip-Gram для обработки текста}
\author{}
\date{}

\begin{document}

\maketitle

\section{Введение}
В этой лабораторной работе мы реализовали модель Skip-Gram для обработки текстовых данных. Модель Skip-Gram позволяет генерировать векторные представления слов (эмбеддинги) и используется для различных задач обработки естественного языка (NLP), таких как анализ текста, классификация и генерация текста.

\section{Предобработка данных}
\subsection{Очистка текста}
На первом шаге мы очистили текст с помощью функции \texttt{preprocess}, которая выполняет несколько операций:
\begin{itemize}
  \item Преобразует знаки препинания в токены (например, точка заменяется на \texttt{<PERIOD>}).
  \item Удаляет слова, встречающиеся в тексте менее 5 раз.
  \item Возвращает список слов в тексте.
\end{itemize}

\subsection{Маппинг слов в индексы}
Для преобразования слов в числовые индексы и обратно мы создали два словаря:
\begin{itemize}
  \item \texttt{vocab\_to\_int}: отображает слова на их индексы.
  \item \texttt{int\_to\_vocab}: отображает индексы обратно на слова.
\end{itemize}
Частотность слов в тексте определяет порядок их индексации, при этом наиболее частые слова получают меньшие индексы.

\subsection{Субдискретизация слов}
Для удаления часто встречающихся слов (например, \texttt{"the"}, \texttt{"of"}) мы применили метод субдискретизации. Это позволяет ускорить обучение модели и уменьшить шум в данных. Слово отбрасывается с вероятностью:
\[
P(w) = \frac{\sqrt{f(w) / t} + 1}{1 + f(w) / t}
\]
где \( f(w) \) — частота слова, а \( t \) — пороговый параметр.

\section{Формирование обучающих пар}
После предобработки данных мы сформировали обучающие пары: каждое слово в тексте использовалось как центральное слово, а окружающие его слова — как контекст. Для этого была реализована функция \texttt{get\_target}, которая выбирает случайные слова в окне вокруг центрального слова. Размер окна контролируется параметром \texttt{window\_size}.

\section{Обучение модели Skip-Gram}
Модель Skip-Gram состоит из двух слоев:
\begin{itemize}
  \item Входной слой представляет собой матрицу эмбеддингов, которая отображает каждое слово в вектор фиксированного размера.
  \item Выходной слой — линейный слой, который предсказывает вероятности появления контекстных слов на основе центрального слова.
\end{itemize}
Обучение происходило с использованием функции потерь \texttt{CrossEntropyLoss} и оптимизации с помощью алгоритма Adam.

\subsection{Код для реализации модели Skip-Gram}
\begin{verbatim}
class SkipGram(nn.Module):
    def __init__(self, vocab_size: int, embed_size: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.linear = nn.Linear(embed_size, vocab_size)

    def forward(self, x: torch.Tensor):
        out = self.linear(self.embed(x))
        return out
\end{verbatim}

\section{Проверка качества модели}
Для проверки качества обученной модели мы использовали косинусное сходство между векторами эмбеддингов. Это позволяет найти слова, которые наиболее похожи на выбранные слова. Для этого была реализована функция \texttt{cosine\_similarity}.

\section{Визуализация результатов}
После обучения модели мы использовали метод \texttt{T-SNE} для визуализации векторных представлений слов. Технология T-SNE позволяет уменьшить размерность векторов и визуализировать их в 2D пространстве, чтобы увидеть, как семантически похожие слова сгруппированы вместе.

\begin{verbatim}
from sklearn.manifold import TSNE
embed_tsne = TSNE().fit_transform(embeddings[:viz_words, :])
\end{verbatim}

\section{Заключение}
В ходе работы была реализована модель Skip-Gram, обучена на текстовых данных и проверена на основе косинусного сходства между векторами эмбеддингов. Модель успешно группирует слова с похожими значениями, что подтверждает эффективность обученной модели для задач обработки естественного языка.

\end{document}
